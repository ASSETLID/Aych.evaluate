\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{listings}
\usepackage[linktocpage,colorlinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{multicol}

\begin{document}

\title{PerfOCaml: a Benchmarking Environment for OCaml\\Specification 2014/06/02}

\maketitle
\tableofcontents
\section{Use Cases}

Here are some examples of how users should be able to use PerfOCaml.

\subsection{Micro-benchmarking in the Compiler Directory}

Allows to quickly evaluate the difference between previous versions or
build of the compiler and the just build (not installed) one.

In the directory where OCaml was built:
\begin{itemize}
\item {\tt operf-micro init}: prepare the environment
\item {\tt operf-micro build}: prepare the benchmarking binaries
\item {\tt operf-micro clean}: reinitialize after a rebuild of the compiler
\item {\tt operf-micro run}: generate the benchmark result files
\item {\tt operf-micro save path}: save results to the path
\item {\tt operf-micro compare path}: compare the results with the runs saved in the path
\end{itemize}

Constraints: should not depend on anything external to the just-built
compiler, should run on windows and should be able to compare
completely unrelated versions of the compiler.

\subsection{Macro-benchmarking in the Compiler Directory}

Provides a more complete idea of the performance of a functionnal
ocaml source tree.

After ensuring that the compiler seems to correctly work and install,
run in the source directory:

\begin{itemize}
\item {\tt operf-ocaml init --(additionnal configure options)}: prepare the environment (create an opam root, switch, ...)
\item {\tt operf-ocaml build [list of benchmark suites] }: build the compiler, the libraries and programs needed to run the selected benchmarks
\item {\tt operf-ocaml run}: run the selected benchmarks and generate the result files
\item {\tt operf-ocaml save path}: save results to the path
\item {\tt operf-ocaml compare path}: compare the results with the runs saved in the path
\item {\tt operf-ocaml clean}: reset to the state after init (can change the compiler source code and launch build with the updated sources).
\end{itemize}

\subsection{Macro-benchmarking an OPAM Switch}

Here, the user wants to benchmark a compiler or a package that is
installed in some existing OPAM switch.

\subsection{Weather service}

The Weather service should be a regularly (automatically) updated web
page comparing different versions of the compilers.

Every day or commit in a compiler branch the compiler should be
rebuilt and run on various sets of benchmarks based on a set of fixed
versions of opam packages (the same for every version of the compiler)

The site should provide graphs of evolutions of differents agregated
performance indices, precise results for each benchmarks, a list of
significative regressions and analysable raw data (like perf records).

Users using exotic architectures or systems should be able to submit
result assotiated with precise informations about the run environment.

\section{Sub-Projects}

\subsection{Micro-benchmarking in the Compiler Directory}

\subsubsection{The {\tt operf-micro} tool}

In the directory {\tt\em \$OCAMLSRC} where OCaml was built:
\begin{itemize}
\item {\tt operf-micro init {\em name}}\\ Prepare the environment
  for micro-benchmarking the compiler in the same directory.
  Do the following operations:
  \begin{itemize}
  \item Create a sub-directory {\tt operf}, with a config file {\tt config}
  indicating that the name of the current compiler is {\tt\em name}.
  {\tt operf-micro} will complain if the directory
  {\tt \$HOME/.operf/micro/{\em name}/} \footnote{the path should probably
    follow XDG recomandations} already exists (as it will try to
  save the micro-benchmarks results in that location).
  \item Download a compressed archive of the micro-benchmark from a
    default URL. A {\tt [--benchmarks {\em url} ]} switch could be
    used to override the default URL, in particular to specify a local
    archive.\footnote{It should probably use the one provided at
      installation}
  \item Extract the archive in the {\tt operf} sub-directory
  \end{itemize}
\item {\tt operf-micro build [--byte | --opt]}\\
  Try to build the benchmarks, using the local compiler. The benchmarks will
  assume that the bytecode compiler is:\\
  {\tt \$OCAMLSRC/byterun/ocamlrun \$OCAMLSRC/ocamlc -I \$OCAMLSRC/stdlib}\\
  and the native code compiler is:\\
  {\tt \$OCAMLSRC/byterun/ocamlrun \$OCAMLSRC/ocamlopt -I \$OCAMLSRC/stdlib}\\
  It might also assume that the {\tt bigarray} library is available.
  This step is useful if some changes have to be done to the benchmarks in
  order for them to correctly compile (note that changing the benchmarks
  can lead to false comparisons).
\item {\tt operf-micro clean}\\
  Reinitialize after a rebuild of the compiler
\item {\tt operf-micro run [--byte | --opt]}\\
  Run the micro-benchmarks. If {\tt \$OCAMLSRC/byterun/ocamlrun},
  {\tt \$OCAMLSRC/ocamlc} or {\tt \$OCAMLSRC/ocamlopt} have changed since the
  last compilation, it will first clean the benchmarks. If needed, it will
  compile the benchmarks.

  Results are stored in {\tt \$HOME/.operf/micro/{\em name}/{\em
      date}/} where {\tt\em date} has the format {\tt\em
    YYYY-MM-DD-hh-mm-ss}.
\item {\tt operf-micro compare {\tt ({\em name} | {\em date} | {\em
      name:date}) }}\\ Compare the latest results with a previous run.
  If {\tt\em name} is provided, it is for another compiler. If {\tt\em
    date} is provided, it is the last run just before or equal to that
  date.
\item {\tt operf-micro save-diff {\em diff-name}}\\
  If modifications where done to the micro-benchmarks, save them in
  {\tt \$HOME/.operf/micro-diffs/{\em diff-name}.patch} for later use.
  An example of meaningful modification would be to replace an old
  construct with a new one only available after a given compiler version.
\item  {\tt operf-micro load-diff {\em diff-name}}\\
  Try to find an existing patch and apply it to the current benchmarks.
\item {\tt operf-micro version {\em ocaml-version ...}}\\ Download OCaml
  sources for version {\tt\em ocaml-version}, build them, create a
  micro-benchmark with name {\tt\em ocaml-version}, and run the
  micro-benchmarks. This can be used to populate the micro-benchmarks
  for comparisons.
\end{itemize}

\subsubsection{Format of the Micro Benchmarks}

A micro benchmark archive contains:

\begin{itemize}
\item {\tt description}: a textual desciprion of what the benchmark does and evaluates
\item {\tt set}: a directory containing benchmark directories
  \begin{itemize}
  \item benchmark\_name
    \begin{itemize}
    \item {\tt main.ml}: containing a {\tt functions} variable
      describing the list of available functions to run
    \item optionnal additionnal .ml files
    \item optionnal {\tt compile} file: describe how to compile the suite If none provided, a
      reasonnable default is assumed
    \end{itemize}
  \end{itemize}
\end{itemize}

{\tt main.ml}
\begin{verbatim}

let f1 () = 1 + 2
let check_f1 v = v = 3

let f2 n = Array.create n 1
let check_f2 n v = Array.length v = n

open Micro_bench_types

let functions =
  [ "function 1",          Unit (f1, check_f1);
    "some other function", Int (f2, check_f2) ]

\end{verbatim}

{\tt compile}
\begin{verbatim}
file1.mli file1.ml file2.ml main.ml
\end{verbatim}
The order field, describe the compile and link order. If no {\tt compile}
file is provided one containing only ``main.ml'' is assumed.
\footnote{should we allow C files ?}

The files will be compiled with the command line:
\begin{verbatim}
ocamlopt micro_bench_type.mli file1.mli file1.ml file2.ml main.ml micro_bench_runner.ml
\end{verbatim}

\subsubsection{Format of the {\tt \$HOME/.operf/} directory}

\begin{itemize}
\item {\tt config} file describing general configuration
\item {\tt sets} directory containing:
  \begin{itemize}
  \item {\tt additionnal} directory with additional benchmark
    suites. The structure is like a benchmark archive
  \item {\tt patch} directory containing .patch files applicable to
    existing suite
  \end{itemize}
\item {\tt results}:
  \begin{itemize}
  \item {\tt last}: a file with the name of the last recorded results
  \item ``compiler name'' directories
    \begin{itemize}
    \item ``revision'' or ``date/time'' directories
      \begin{itemize}
      \item {\tt system\_info}
      \item ``suite name''.result
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{verbatim}
{ name: "farm runner 1",
  cpu: { architecture: "amd64",
         count: 4,
         ... },
  memory: { size: 4096, ... },
  os: { virtualized: true,
        kind: "Linux",
        ... } }
\end{verbatim}

\subsubsection{Format of the Result Files}

\begin{verbatim}
{ name: "benchmark name",
  ocaml-version: "4.02.1+hash",
  date: "Mon Jun  2 15:27:43 CEST 2014",
  computer-description:
    { architecure: "amd64",
      model: "...",
      system: "Linux 3.13-1-amd64",
      ... },
  runs:
    [ { name: "some function",
        description: "...",
        list:
          [ { count: 1000, time: 0.00123, cycles: 1234567,
              minor-words: 123, major-words: 1,
              compactions: 0, ... },
            { time: 132, ... },
            ... ] },
      ... ] }
\end{verbatim}

\subsubsection{Format of the Comparison Results}

A human readable result. Either textual statistical informations or a graph.
~\vspace{4cm}~
\subsection{Macro-benchmarking in the Compiler Directory}

\subsubsection{The {\tt operf-ocaml} tool}
~\vspace{4cm}~

\subsubsection{Format of the Macro Benchmarks}
~\vspace{4cm}~

\subsubsection{Format of the {\tt \$HOME/.operf/macro/} directory}
~\vspace{4cm}~

\subsubsection{Format of the Result Files}

The same format as micro benchmark results, but run fields may differ
depending on what is available and macro benchmark inputs.
~\vspace{4cm}~

\subsection{Macro-benchmarking an OPAM Switch}

\subsubsection{The {\tt operf-opam} tool}

This tool has the same behavior as {\tt operf-ocaml}, except that,
instead of being used in a compiler directory, it will install and run
the macro benchmarks on the current OPAM switch (or one provided on
the command line).

\begin{itemize}
\item {\tt operf-opam [--switch {\em OPAM-SWITCH} ] install {\em
    benchmark} [...]}\\ Install the specified {\tt\em benchmarks} in
  the current switch (or {\tt\em OPAM-SWITCH} if provided).
\item {\tt operf-opam [--switch {\em OPAM-SWITCH} ] run [{\em benchmark} ...]}\\
  Run the installed {\tt\em benchmarks} (or all the installed one, if none
  is specified) and save the corresponding results.
\end{itemize}

\subsection{Weather service}
~\vspace{4cm}~

\section{Miscellaneous Ideas}

Provides a more complete idea of the performance of a functionnal
ocaml source tree.

Basic aim:
\begin{itemize}
\item Micro-benchmarking should be as fast and simple as possible.
\item Global benchmarking and weather service should share their sets
  of benchmarks. This set should be easy to extend.
\item Weather services should require as few maintenance as possible.
\end{itemize}

There are two distinct kinds of benchmarks, \emph{micro-benchmarks} (a specific
function) and \emph{macro-benchmarks} (a program).

\subsection{Description of a global benchmarks}

A benchmark is a specific OPAM package, it is not included in the
library or program package. Running is done by installing the
benchmark package. The install instruction copy the resulting data to
a directory fixed by convention. (probably
\texttt{\%\{prefix\}\%/bench/package\_name})

A package generating benchmark results should be annotated with the
\texttt{benchmark} tag.

\begin{itemize}
\item No or minimal change to opam are required.
\item Allow to add additionnal dependencies to the benchmark that are
  not required by the original package.
\end{itemize}

Potential problems:
\begin{itemize}
\item Benchmarking functions not available in a module interface is
  not directly possible. It can be circumvented by having a benchmark
  package containing the library sources (more package management
  work), or by exposing a benchmarkable value in the interfaces (can
  be done only by the library develloper).
\end{itemize}

\subsubsection{Usefull opam extension}

Another way to specify a benchmark could be to change opam packages to
allow multiple packages to share the same description. This could be
done through the addition of {\tt targets} to the opam file.

For instance, given the following original package:

\begin{verbatim}
build: [
  ["./configure" "--prefix" prefix]
  [make]
  [make "install"]
]
remove: [["ocamlfind" "remove" "mylib"]]
depends: ["ocamlfind" "library1" "library2"]
\end{verbatim}

Adding the target doc and benchmark:

\begin{verbatim}
targets: ["documentation", "benchmark"]
build: [
  ["./configure" "--prefix" prefix]
  [make]
  [make "install"]
]
depends: ["ocamlfind" "library1" "library2"]
remove: [["ocamlfind" "remove" "mylib"]]

build-documentation: [
  ["./configure" "--prefix" prefix]
  [make doc]
  [make "install-doc"]
]
depends-documentation: ["ocamlfind" "library1" "library2" "ocamldoc"]
remove-documentation: [[make "uninstall-doc"]]

build-benchmark: [
  ["./configure" "--prefix" prefix]
  [make]
  [make bench]
  ["cp" "-r" "bench_result" "%{share}%/benchmark/%{package-name}%/%{version}%"]
]
depends-benchmark: ["ocamlfind" "library1" "library2" {>= 1.3} "core_bench"]
remove-benchmark: ["rm" "-rf" "%{share}%/benchmark/%{package-name}%/%{version}%"]
\end{verbatim}

This single opam file would describe, the package ``mylib'' but also
``mylib-documentation'' and ``mylib-benchmark''.

To distinguish with a specified ``mylib-benchmark'' package and allow
to chose which one to install (with opam pin), the version of this
kind of package whould be suffixed with ``+target'', and discret
packages with this suffix would be forbidden. To ensure that the
``-target'' packages are not installed with a different version of the
library or program, implicit conflicts are added (the same conflicts
that prevents two versions of a package to be installed at the same
time).

Note: it could be also be interesting to separate the ``./configure'',
``make'' and ``make install'' part of the build command in three
subcommands.

Advantages
\begin{itemize}
\item Smaller opam repository maintenance burden (less files to edit).
\item Could serve to also handle tests: The problem with testing is
  that it is often easier to run them in the directory where the
  library or program was built, because some tested functions could be
  hidden by interfaces or some usefull data are kept in the source
  tree. With this proposition, it is simple to define such testing
  packages without duplicating package descriptions.

  Compared to the current ``build-test'' opam feature it have a saner
  dependency semantics, it does not force to rebuild all the package
  depending on the tested package, and allows to have a completely
  different configuration command than the build (enabling inline
  tests, high verbosity, ...).
\end{itemize}

There is no loss of flexibility: if a completely different tarball is
used for benchmarking, it is still possible to hand-define a
benchmarking package like in the previous proposition.

Potential problems:
\begin{itemize}
\item Increased compilation time when systematically building the
  documentation. There may be some reasonnable solutions to build the
  documentation where the library has been built in some common cases.
\item more changes to opam
\end{itemize}

\subsection{Macro-benchmark Runner}

Input:
\begin{itemize}
\item a binary path
\item expected result
\item variables, their encoding and potential values
\end{itemize}

Output:
\begin{itemize}
\item A benchmark file.
\item Additionnal informations to analyse the results provided by
  tools like \texttt{perf record} on linux or \texttt{callgrind}
\end{itemize}

Note: Reporting running time can be done simply in a cross platform
way.

Reporting more precise result like cycles is easy on linux but
other systems would require some search. GC statistics requires
changes to the original program. A possibility would be to addopt as
convention that the environment variable
\texttt{OCAML\_GC\_STATISTICS} contains a file where statistics should
be written (using Gc.print\_stat).

Defining a macrobenchmark package of a program built by an existing
opam package should not requires to provides additionnal sources (a
url file in the opam package).

\subsection{Microbenchmark runner}

Runs a set of functions, test the results, records the
timings/cycles/gc informations and write raw results to the directory
fixed by convention.

Probably the runner part of core\_bench. This should be extracted from
core\_bench (3 files) to minimize dependencies, especialy camlp4.

This should be provided as a template package, already containing
build system and base files. The user should only provide a .ml file
containing a list of functions to test and a file containing the list
of build dependencies (the ocamlfind -package part of the compilations
command).

This should allow to provide a microbenchmark as a simple opam package
containing those 2 files in the \texttt{files} subdirectory.

\subsection{Simple microbenchmark set}

A set of microbenchmarks able to run everywhere without any difference
should be provided to reliably test the compiler. It should not depend
on any kind of code generation, or feaure specific to a version.

\subsection{Management}

\begin{itemize}
\item List packages containing benchmarks
\item Prepare an opam switch to install a manualy built compiler
\item One command compile/run/recover/clean for a given opam compiler
  package and set of benchmarks.
\end{itemize}

\subsection{Agregation}

This tool should produce the set of graphs given a set of raw
results. It will probably use core\_bench for the statistics part. It
should produce the final html pages to display on the public site.

\subsection{Results management}

Use github: no tool needed.

Benchmark results should be managed in a centralised git repositories,
probably using github to minimize maintenance. Users publishing
results on their exotic systems should send patches/pull requests.

Common architectures results should always be provided by the same
computers to allow easier comparisons, all results should be
automaticaly produced and commited to the repository.

\section{Related work}

\href{http://speed.pypy.org/}{pypy speed site}

\end{document}
