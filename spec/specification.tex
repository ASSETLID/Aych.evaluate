\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{listings}
\usepackage[linktocpage,colorlinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{multicol}

\begin{document}

\title{PerfOCaml: a Benchmarking Environment for OCaml\\Specification 2014/05/27}

\maketitle

\section{Use Cases}

Some examples of what users should be able to do with PerfOCaml.

Those are in this order, the expected milestones:

\subsection{Micro-benchmarks for local testing}

Allows to quickly evaluate the difference between previous versions or
build of the compiler and the just build (not installed) one.

In the directory where OCaml was built:
\begin{itemize}
\item {\tt ocp-perf-compiler init}: prepare the environment
\item {\tt ocp-perf-compiler build}: prepare the benchmarking binaries
\item {\tt ocp-perf-compiler clean}: reinitialize after a rebuild of the compiler
\item {\tt ocp-perf-compiler run}: generate the benchmark result files
\item {\tt ocp-perf-compiler save path}: save results to the path
\item {\tt ocp-perf-compiler compare path}: compare the results with the runs saved in the path
\end{itemize}

Constraints: should not depend on anything external to the just-built
compiler, should run on windows and should be able to compare
completely unrelated versions of the compiler.

\subsection{Global benchmarks for local testing}

Provides a more complete idea of the performance of a functionnal
ocaml source tree.

After ensuring that the compiler seems to correctly work and install,
run in the source directory:

\begin{itemize}
\item {\tt ocp-perf-ocaml init --(additionnal configure options)}: prepare the environment (create an opam root, switch, ...)
\item {\tt ocp-perf-ocaml build [list of benchmark suites] }: build the compiler, the libraries and programs needed to run the selected benchmarks
\item {\tt ocp-perf-ocaml run}: run the selected benchmarks and generated the result files
\item {\tt ocp-perf-ocaml save path}: save results to the path
\item {\tt ocp-perf-ocaml compare path}: compare the results with the runs saved in the path
\item {\tt ocp-perf-ocaml clean}: reset to the state after init (can change the compiler source code and launch build with the updated sources).
\end{itemize}

\subsection{Weather service}

The Weather service should be a regularly (automatically) updated web
page comparing different versions of the compilers.

Every day or commit in a compiler branch the compiler should be
rebuilt and run on various sets of benchmarks based on a set of fixed
versions of opam packages (the same for every version of the compiler)

The site should provide graphs of evolutions of differents agregated
performance indices, precise results for each benchmarks, a list of
significative regressions and analysable raw data (like perf records).

Users using exotic architectures or systems should be able to submit
result assotiated with precise informations about the run environment.

\section{Proposition}

Basic aim:
\begin{itemize}
\item Micro-benchmarking should be as fast and simple as possible.
\item Global benchmarking and weather service should share their sets
  of benchmarks. This set should be easy to extend.
\item Weather services should require as few maintenance as possible.
\end{itemize}

There are two distinct kinds of benchmarks, \emph{micro-benchmarks} (a specific
function) and \emph{macro-benchmarks} (a program).

\subsection{Description of a global benchmarks}

A benchmark is a specific OPAM package, it is not included in the
library or program package. Running is done by installing the
benchmark package. The install instruction copy the resulting data to
a directory fixed by convention. (probably
\texttt{\%\{prefix\}\%/bench/package\_name})

A package generating benchmark results should be annotated with the
\texttt{benchmark} tag.

\begin{itemize}
\item No or minimal change to opam are required.
\item Allow to add additionnal dependencies to the benchmark that are
  not required by the original package.
\end{itemize}

Potential problems:
\begin{itemize}
\item Benchmarking functions not available in a module interface is
  not directly possible. It can be circumvented by having a benchmark
  package containing the library sources (more package management
  work), or by exposing a benchmarkable value in the interfaces (can
  be done only by the library develloper).
\end{itemize}

\subsection{Benchmark result format}

Not decided, probably a simple json or csv like
format\footnote{Fabrice: it must be decided now. Any existing standard
  in other languages ?}

\section{Organisation in Sub-Projects}

Comment\footnote{Fabrice: we should divide the global project into
  several sub-projects, such as (1) local bench-marking a package, (2)
  local bench-marking the compiler, (3) community website and (4)
  OCaml performance weather server}

\subsubsection{System informations}

Recover as much relevant system informations as possible:
\begin{itemize}
\item Hardware informations: CPU, memory, ...
\item System informations: versions of OS, libraries, virtualisation, ...
\end{itemize}

\subsubsection{Macro-benchmark Runner}

Input:
\begin{itemize}
\item a binary path
\item expected result
\item variables, their encoding and potential values
\end{itemize}

Output:
\begin{itemize}
\item A benchmark file.
\item Additionnal informations to analyse the results provided by
  tools like \texttt{perf record} on linux or \texttt{callgrind}
\end{itemize}

Note: Reporting running time can be done simply in a cross platform
way.

Reporting more precise result like cycles is easy on linux but
other systems would require some search. GC statistics requires
changes to the original program. A possibility would be to addopt as
convention that the environment variable
\texttt{OCAML\_GC\_STATISTICS} contains a file where statistics should
be written (using Gc.print\_stat).

Defining a macrobenchmark package of a program built by an existing
opam package should not requires to provides additionnal sources (a
url file in the opam package).

\subsubsection{Microbenchmark runner}

Runs a set of functions, test the results, records the
timings/cycles/gc informations and write raw results to the directory
fixed by convention.

Probably the runner part of core\_bench. This should be extracted from
core\_bench (3 files) to minimize dependencies, especialy camlp4.

This should be provided as a template package, already containing
build system and base files. The user should only provide a .ml file
containing a list of functions to test and a file containing the list
of build dependencies (the ocamlfind -package part of the compilations
command).

This should allow to provide a microbenchmark as a simple opam package
containing those 2 files in the \texttt{files} subdirectory.

\subsubsection{Simple microbenchmark set}

A set of microbenchmarks able to run everywhere without any difference
should be provided to reliably test the compiler. It should not depend
on any kind of code generation, or feaure specific to a version.

\subsubsection{Management}

\begin{itemize}
\item List packages containing benchmarks
\item Prepare an opam switch to install a manualy built compiler
\item One command compile/run/recover/clean for a given opam compiler
  package and set of benchmarks.
\end{itemize}

\subsubsection{Agregation}

This tool should produce the set of graphs given a set of raw
results. It will probably use core\_bench for the statistics part. It
should produce the final html pages to display on the public site.

\subsubsection{Results management}

Use github: no tool needed.

Benchmark results should be managed in a centralised git repositories,
probably using github to minimize maintenance. Users publishing
results on their exotic systems should send patches/pull requests.

Common architectures results should always be provided by the same
computers to allow easier comparisons, all results should be
automaticaly produced and commited to the repository.

\section{Milestones}

Comment\footnote{Fabrice: we should define some initial milestones for
  every sub-project}

\end{document}
