\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{listings}
\usepackage[linktocpage,colorlinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{multicol}

\begin{document}

\title{PerfOCaml: a Benchmarking Environment for OCaml\\Specification 2014/06/02}

\maketitle
\tableofcontents
\section{Use Cases}

Here are some examples of how users should be able to use PerfOCaml.

\subsection{Micro-benchmarking in the Compiler Directory}

Allows to quickly evaluate the difference between previous versions or
build of the compiler and the just build (not installed) one.

In the directory where OCaml was built:
\begin{itemize}
\item {\tt operf-micro init}: prepare the environment
\item {\tt operf-micro build}: prepare the benchmarking binaries
\item {\tt operf-micro clean}: reinitialize after a rebuild of the compiler
\item {\tt operf-micro run}: generate the benchmark result files
\item {\tt operf-micro save path}: save results to the path
\item {\tt operf-micro compare path}: compare the results with the runs saved in the path
\end{itemize}

Constraints: should not depend on anything external to the just-built
compiler, should run on windows and should be able to compare
completely unrelated versions of the compiler.

\subsection{Macro-benchmarking in the Compiler Directory}

Provides a more complete idea of the performance of a functionnal
ocaml source tree.

After ensuring that the compiler seems to correctly work and install,
run in the source directory:

\begin{itemize}
\item {\tt operf-ocaml init --(additionnal configure options)}: prepare the environment (create an opam root, switch, ...)
\item {\tt operf-ocaml build [list of benchmark suites] }: build the compiler, the libraries and programs needed to run the selected benchmarks
\item {\tt operf-ocaml run}: run the selected benchmarks and generate the result files
\item {\tt operf-ocaml save path}: save results to the path
\item {\tt operf-ocaml compare path}: compare the results with the runs saved in the path
\item {\tt operf-ocaml clean}: reset to the state after init (can change the compiler source code and launch build with the updated sources).
\end{itemize}

\subsection{Macro-benchmarking an OPAM Switcha}

Here, the user wants to benchmark a compiler or a package that is
installed in some existing OPAM switch.

\subsection{Weather service}

The Weather service should be a regularly (automatically) updated web
page comparing different versions of the compilers.

Every day or commit in a compiler branch the compiler should be
rebuilt and run on various sets of benchmarks based on a set of fixed
versions of opam packages (the same for every version of the compiler)

The site should provide graphs of evolutions of differents agregated
performance indices, precise results for each benchmarks, a list of
significative regressions and analysable raw data (like perf records).

Users using exotic architectures or systems should be able to submit
result assotiated with precise informations about the run environment.

\section{Sub-Projects}

\subsection{Micro-benchmarking in the Compiler Directory}

\subsubsection{The {\tt operf-micro} tool}

In the directory {\tt\em \$OCAMLSRC} where OCaml was built:
\begin{itemize}
\item {\tt operf-micro init {\em name}}\\ Prepare the environment
  for micro-benchmarking the compiler in the same directory.
  Do the following operations:
  \begin{itemize}
  \item Create a sub-directory {\tt operf}, with a config file {\tt config}
  indicating that the name of the current compiler is {\tt\em name}.
  {\tt operf-micro} will complain if the directory
  {\tt \$HOME/.operf/micro/{\em name}/} already exists (as it will try to
  save the micro-benchmarks results in that location).
  \item Download a compressed archive of the micro-benchmark from a
    default URL. A {\tt [--benchmarks {\em url} ]} switch could be
    used to override the default URL, in particular to specify a local
    archive.
  \item Extract the archive in the {\tt operf} sub-directory
  \end{itemize}
\item {\tt operf-micro build [--byte | --opt]}\\
  Try to build the benchmarks, using the local compiler. The benchmarks will
  assume that the bytecode compiler is:\\
  {\tt \$OCAMLSRC/byterun/ocamlrun \$OCAMLSRC/ocamlc -I \$OCAMLSRC/stdlib}\\
  and the native code compiler is:\\
  {\tt \$OCAMLSRC/byterun/ocamlrun \$OCAMLSRC/ocamlopt -I \$OCAMLSRC/stdlib}\\
  It might also assume that the {\tt bigarray} library is available.
  This step is useful if some changes have to be done to the benchmarks in
  order for them to correctly compile (note that changing the benchmarks
  can lead to false comparisons).
\item {\tt operf-micro clean}\\
  Reinitialize after a rebuild of the compiler
\item {\tt operf-micro run [--byte | --opt]}\\
  Run the micro-benchmarks. If {\tt \$OCAMLSRC/byterun/ocamlrun},
  {\tt \$OCAMLSRC/ocamlc} or {\tt \$OCAMLSRC/ocamlopt} have changed since the
  last compilation, it will first clean the benchmarks. If needed, it will
  compile the benchmarks.

  Results are stored in {\tt \$HOME/.operf/micro/{\em name}/{\em
      date}/} where {\tt\em date} has the format {\tt\em
    YYYY-MM-DD-hh-mm-ss}.
\item {\tt operf-micro compare {\tt ({\em name} | {\em date} | {\em
      name:date}) }}\\ Compare the latest results with a previous run.
  If {\tt\em name} is provided, it is for another compiler. If {\tt\em
    date} is provided, it is the last run just before or equal to that
  date.
\item {\tt operf-micro save-diff {\em diff-name}}\\
  If modifications where done to the micro-benchmarks, save them in
  {\tt \$HOME/.operf/micro-diffs/{\em diff-name}.patch} for later use.
  An example of meaningful modification would be to replace an old
  construct with a new one only available after a given compiler version.
\item  {\tt operf-micro load-diff {\em diff-name}}\\
  Try to find an existing patch and apply it to the current benchmarks.
\item {\tt operf-micro version {\em ocaml-version ...}}\\ Download OCaml
  sources for version {\tt\em ocaml-version}, build them, create a
  micro-benchmark with name {\tt\em ocaml-version}, and run the
  micro-benchmarks. This can be used to populate the micro-benchmarks
  for comparisons.
\end{itemize}

\subsubsection{Format of the Micro Benchmarks}

\subsubsection{Format of the {\tt \$HOME/.operf/} directory}

\subsubsection{Format of the Result Files}

\subsubsection{Format of the Comparison Results}

\subsection{Macro-benchmarking in the Compiler Directory}

\subsubsection{The {\tt operf-ocaml} tool}

\subsubsection{Format of the Macro Benchmarks}

\subsubsection{Format of the {\tt \$HOME/.operf/macro/} directory}

\subsubsection{Format of the Result Files}

\subsection{Macro-benchmarking an OPAM Switch}

\subsubsection{The {\tt operf-opam} tool}

This tool has the same behavior as {\tt operf-ocaml}, except that,
instead of being used in a compiler directory, it will install and run
the macro benchmarks on the current OPAM switch (or one provided on
the command line).

\begin{itemize}
\item {\tt operf-opam [--switch {\em OPAM-SWITCH} ] install {\em
    benchmark} [...]}\\ Install the specified {\tt\em benchmarks} in
  the current switch (or {\tt\em OPAM-SWITCH} if provided).
\item {\tt operf-opam [--switch {\em OPAM-SWITCH} ] run [{\em benchmark} ...]}\\
  Run the installed {\tt\em benchmarks} (or all the installed one, if none
  is specified) and save the corresponding results.
\end{itemize}

\subsection{Weather service}

\section{Miscellaneous Ideas}

Provides a more complete idea of the performance of a functionnal
ocaml source tree.


Basic aim:
\begin{itemize}
\item Micro-benchmarking should be as fast and simple as possible.
\item Global benchmarking and weather service should share their sets
  of benchmarks. This set should be easy to extend.
\item Weather services should require as few maintenance as possible.
\end{itemize}

There are two distinct kinds of benchmarks, \emph{micro-benchmarks} (a specific
function) and \emph{macro-benchmarks} (a program).

\subsection{Description of a global benchmarks}

A benchmark is a specific OPAM package, it is not included in the
library or program package. Running is done by installing the
benchmark package. The install instruction copy the resulting data to
a directory fixed by convention. (probably
\texttt{\%\{prefix\}\%/bench/package\_name})

A package generating benchmark results should be annotated with the
\texttt{benchmark} tag.

\begin{itemize}
\item No or minimal change to opam are required.
\item Allow to add additionnal dependencies to the benchmark that are
  not required by the original package.
\end{itemize}

Potential problems:
\begin{itemize}
\item Benchmarking functions not available in a module interface is
  not directly possible. It can be circumvented by having a benchmark
  package containing the library sources (more package management
  work), or by exposing a benchmarkable value in the interfaces (can
  be done only by the library develloper).
\end{itemize}

\subsection{Benchmark result format}

Not decided, probably a simple json or csv like
format\footnote{Fabrice: it must be decided now. Any existing standard
  in other languages ?}

\subsection{System informations}

Recover as much relevant system informations as possible:
\begin{itemize}
\item Hardware informations: CPU, memory, ...
\item System informations: versions of OS, libraries, virtualisation, ...
\end{itemize}

\subsection{Macro-benchmark Runner}

Input:
\begin{itemize}
\item a binary path
\item expected result
\item variables, their encoding and potential values
\end{itemize}

Output:
\begin{itemize}
\item A benchmark file.
\item Additionnal informations to analyse the results provided by
  tools like \texttt{perf record} on linux or \texttt{callgrind}
\end{itemize}

Note: Reporting running time can be done simply in a cross platform
way.

Reporting more precise result like cycles is easy on linux but
other systems would require some search. GC statistics requires
changes to the original program. A possibility would be to addopt as
convention that the environment variable
\texttt{OCAML\_GC\_STATISTICS} contains a file where statistics should
be written (using Gc.print\_stat).

Defining a macrobenchmark package of a program built by an existing
opam package should not requires to provides additionnal sources (a
url file in the opam package).

\subsection{Microbenchmark runner}

Runs a set of functions, test the results, records the
timings/cycles/gc informations and write raw results to the directory
fixed by convention.

Probably the runner part of core\_bench. This should be extracted from
core\_bench (3 files) to minimize dependencies, especialy camlp4.

This should be provided as a template package, already containing
build system and base files. The user should only provide a .ml file
containing a list of functions to test and a file containing the list
of build dependencies (the ocamlfind -package part of the compilations
command).

This should allow to provide a microbenchmark as a simple opam package
containing those 2 files in the \texttt{files} subdirectory.

\subsection{Simple microbenchmark set}

A set of microbenchmarks able to run everywhere without any difference
should be provided to reliably test the compiler. It should not depend
on any kind of code generation, or feaure specific to a version.

\subsection{Management}

\begin{itemize}
\item List packages containing benchmarks
\item Prepare an opam switch to install a manualy built compiler
\item One command compile/run/recover/clean for a given opam compiler
  package and set of benchmarks.
\end{itemize}

\subsection{Agregation}

This tool should produce the set of graphs given a set of raw
results. It will probably use core\_bench for the statistics part. It
should produce the final html pages to display on the public site.

\subsection{Results management}

Use github: no tool needed.

Benchmark results should be managed in a centralised git repositories,
probably using github to minimize maintenance. Users publishing
results on their exotic systems should send patches/pull requests.

Common architectures results should always be provided by the same
computers to allow easier comparisons, all results should be
automaticaly produced and commited to the repository.

\end{document}
